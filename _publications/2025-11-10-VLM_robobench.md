---
title: "AgenticLab: A Real-world Robot Agent Platform that Can See, Think, and Act"

permalink: /publication/2026-01-31-AgenticLab
excerpt: '<br/><a href="https://drive.google.com/file/d/1IxNuYt15cYBI_d9qOa1-s4-BhJElBFpb/view?usp=drive_link" target="_blank"><video autoplay loop muted playsinline style="width: 100%; max-width: 400px; border-radius: 8px;"><source src="/files/AgenticLab_teaser.mp4" type="video/mp4"></video></a><br/><br/><a href="https://arxiv.org/pdf/2602.01662" class="btn btn--info"><i class="fas fa-file-pdf"></i> arXiv</a> <a href="#" class="btn btn--info"><i class="fas fa-video"></i> Website(Soon)</a> <a href="#" class="btn btn--info"><i class="fab fa-github"></i> Code(Soon)</a> <a href="#" class="btn btn--info" onclick="copyBibtex(); return false;"><i class="fas fa-book"></i> BibTeX</a>'
date: 2026-01-31
venue: Under Review
---

<a href="https://arxiv.org/pdf/2602.01662" class="btn btn--info"><i class="fas fa-file-pdf"></i> arXiv</a>
<a href="#" class="btn btn--info"><i class="fas fa-video"></i> Website(Soon)</a>
<a href="#" class="btn btn--info"><i class="fab fa-github"></i> Code(Soon)</a>
<a href="#" class="btn btn--info" onclick="copyBibtex(); return false;"><i class="fas fa-book"></i> BibTeX</a>

<pre id="bibtex-citation" style="display: none;">@misc{guo2026agenticlab,
  title={AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act},
  author={Pengyuan Guo and Zhonghao Mai and Zhengtong Xu and Kaidi Zhang and Heng Zhang and Zichen Miao and Arash Ajoudani and Zachary Kingston and Qiang Qiu and Yu She},
  year={2026},
  eprint={2602.01662},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2602.01662},
}</pre>

<script>
function copyBibtex() {
  var bibtex = document.getElementById('bibtex-citation').textContent;
  navigator.clipboard.writeText(bibtex).then(function() {
    alert('BibTeX citation copied to clipboard!');
  }, function(err) {
    console.error('Could not copy text: ', err);
  });
}
</script>

## Overview

In this work, we present **AgenticLab**, an **open-source, real-world benchmark platform** that integrates large vision-language models (VLMs) and large language models (LLMs) to enable _zero-shot robotic manipulation_ through reasoning and embodied interaction.

Unlike existing simulation-based benchmarks, AgenticLab focuses on **accessible hardware**, **Open Knowledge-based Models**, and **compositional agentic intelligence**, allowing researchers to study how VLM/LLM models perform in real-world manipulation scenarios.

- <strong style="font-size: 1.2em;">[More Videos (in-the-wild)](https://drive.google.com/drive/folders/1FhkyPC2_33m97-Z4PHx9VMXpbwQS6fPe?usp=sharing)</strong>
